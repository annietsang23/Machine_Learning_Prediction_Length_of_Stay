{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split,KFold,cross_val_score,GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build first set of models using Cross Validation and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decorator for calculating processing time\n",
    "def timer(f):\n",
    "    def wrapper(*args,**kwargs):\n",
    "        start=time.time()\n",
    "        x=f(*args,**kwargs)\n",
    "        end=time.time()\n",
    "        t=datetime.datetime.fromtimestamp(end-start).strftime('%Mm %ss %fms')\n",
    "        print(f'{f.__name__}- processing time: {t}')\n",
    "        return x,(end-start)\n",
    "    return wrapper\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform PCA transformation\n",
    "def pca_transform(df):\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca=PCA()\n",
    "    pca.fit(df)\n",
    "    arr=np.cumsum(pca.explained_variance_ratio_)\n",
    "    #select the number of components which add up to a variance ratio of 1\n",
    "    n=len(arr[arr<1])\n",
    "    \n",
    "    pca=PCA(n_components=n)\n",
    "    df_t=pca.fit_transform(df)\n",
    "    return df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Retrieve saved X and y data and split the dataset into training and testing sets\n",
    "def data_split(d):\n",
    "    df=d.copy()\n",
    "    X_los=df.drop(['los_days'],axis=1)\n",
    "    y_los=df['los_days']\n",
    "\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X_los,y_los,test_size=0.2,random_state=0)\n",
    "    \n",
    "    return X_train,X_test,y_train,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform features into polynomial feature map and then perform linear regression\n",
    "def build_pr(X_train,y_train,score,degree_range):\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    best_score=-2 if score=='r2' else 3000\n",
    "    t_all=0\n",
    "    for i in degree_range:\n",
    "        polynomial_features = PolynomialFeatures(degree=i)\n",
    "        X_train_pr=polynomial_features.fit_transform(X_train)\n",
    "        (m,d_score),t=build_lr(X_train_pr,y_train,score,pr=True)\n",
    "        if score=='r2':\n",
    "            if (d_score>best_score) and (d_score<1):\n",
    "                best_score=d_score\n",
    "                best_model=m\n",
    "        else:\n",
    "            if d_score<best_score:\n",
    "                best_score=d_score\n",
    "                best_model=m\n",
    "        t_all+=t\n",
    "    return (best_model,best_score),t_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "# Linear Regression: Ridge Regression\n",
    "def build_lr(X_train,y_train,score,pr=False):\n",
    "    from sklearn.linear_model import LinearRegression, Ridge\n",
    "    lr=LinearRegression()\n",
    "    lr.fit(X_train,y_train)\n",
    "    params_lr={'alpha':[0.2,0.5,1.0,2,4,10,20,25]}\n",
    "    cv_lr=GridSearchCV(Ridge(),params_lr,cv=5,scoring=score)\n",
    "    cv_lr.fit(X_train,y_train)\n",
    "    best_lr=cv_lr.best_estimator_\n",
    "    best_score_lr=-1*cv_lr.best_score_\n",
    "    if pr is False:\n",
    "        print(best_lr)\n",
    "        print(best_score_lr)\n",
    "    return best_lr,best_score_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def build_rf(X_train,y_train,score):\n",
    "    # Random Forest Regression\n",
    "    params_rf= {'bootstrap':[True],\n",
    "                  'n_estimators': [50,100,200], \n",
    "                  'max_features': ['log2', 'sqrt','auto'], \n",
    "                    'max_depth': [3,5,7], \n",
    "                  'min_samples_split': [2, 3],\n",
    "                    'min_samples_leaf': [8,10]\n",
    "                 }\n",
    "    cv_rf=GridSearchCV(RandomForestRegressor(),params_rf,cv=5,scoring=score)\n",
    "    cv_rf.fit(X_train,y_train)\n",
    "    best_rf=cv_rf.best_estimator_\n",
    "    best_score_rf=-1*cv_rf.best_score_\n",
    "    print(best_rf)\n",
    "    print(best_score_rf)\n",
    "    return best_rf,best_score_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boost Regression\n",
    "@timer\n",
    "def build_gbr(X_train,y_train,score):\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    params_gb={'n_estimators':[50,100,200],\n",
    "                'learning_rate':[0.01,0.05,0.1],\n",
    "                'max_depth':[5,7,10],\n",
    "                'min_samples_split':[2,3],\n",
    "                'min_samples_leaf':[3,5]}\n",
    "    cv_gb=GridSearchCV(GradientBoostingRegressor(),params_gb,cv=5,scoring=score)\n",
    "    cv_gb.fit(X_train,y_train)\n",
    "    best_gb=cv_gb.best_estimator_\n",
    "    best_score_gb=-1*cv_gb.best_score_\n",
    "    print(best_gb)\n",
    "    print(best_score_gb)\n",
    "    return best_gb,best_score_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM Regression\n",
    "@timer\n",
    "def build_svr(X_train,y_train,score):\n",
    "    from sklearn.svm import SVR\n",
    "    params_svr={\n",
    "               'degree':[1,3,5],\n",
    "                'gamma':['scale'],\n",
    "                'C':[1.0,1.5,2],\n",
    "    #             'C':10. ** np.arange(-3, 8),\n",
    "                'epsilon':[0.1, 0.5,1]}\n",
    "    cv_svr=GridSearchCV(SVR(),params_svr,cv=5,scoring=score)\n",
    "    cv_svr.fit(X_train,y_train)\n",
    "    best_svr=cv_svr.best_estimator_\n",
    "    best_score_svr=cv_svr.best_score_\n",
    "    print(best_svr)\n",
    "    print(best_score_svr)\n",
    "    return best_svr,best_score_svr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_depth = 5 : This should be between 3-10. Iâ€™ve started with 5 but you can choose a different number as well. 4-6 can be good starting points.\n",
    "min_child_weight = 1 : A smaller value is chosen because it is a highly imbalanced class problem and leaf nodes can have smaller size groups.\n",
    "gamma = 0 : A smaller value like 0.1-0.2 can also be chosen for starting. This will anyways be tuned later.\n",
    "subsample, colsample_bytree = 0.8 : This is a commonly used used start value. Typical values range between 0.5-0.9.\n",
    "scale_pos_weight = 1: Because of high class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def build_xgb(X_train,y_train,score):\n",
    "    import xgboost as xgb\n",
    "    params_xgb={'n_estimators':[50,100,200],\n",
    "                'learning_rate':[0.01,0.05,0.1],\n",
    "                'max_depth':[3,5,7],\n",
    "                'min_samples_split':[2,3],\n",
    "                'min_samples_leaf':[3,5]}\n",
    "    cv_xgb=GridSearchCV(xgb.XGBRegressor(),params_xgb,cv=5,scoring=score)\n",
    "    cv_xgb.fit(X_train,y_train)\n",
    "    best_xgb=cv_xgb.best_estimator_\n",
    "    best_score_xgb=-1*cv_xgb.best_score_\n",
    "    print(best_xgb)\n",
    "    print(best_score_xgb)\n",
    "    return best_xgb,best_score_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(X_train,y_train,score,degree_range):\n",
    "    model_result={}\n",
    "    processing_time={}\n",
    "    model_result['lr'],processing_time['lr']=build_lr(X_train,y_train,score)\n",
    "    model_result['pr'],processing_time['pr']=build_pr(X_train,y_train,score,degree_range)\n",
    "    model_result['rf'],processing_time['rf']=build_rf(X_train,y_train,score)\n",
    "    model_result['gbr'],processing_time['gbr']=build_gbr(X_train,y_train,score)\n",
    "    model_result['xgb'],processing_time['xgb']=build_xgb(X_train,y_train,score)\n",
    "    model_result['svr'],processing_time['svr']=build_svr(X_train,y_train,score)\n",
    "    return model_result,processing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_accuracy(model,X_test,y_test):    \n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    results = []\n",
    "    prediction = model.predict(X_test)\n",
    "    diff=np.absolute(y_test-prediction)\n",
    "    mse=mean_squared_error(y_test,prediction)\n",
    "    print(f'for a length of {y_test_all.size} the total inaccuracy is {np.sum(y_test)}')\n",
    "    print(f'average inaccuracy is: {np.average(diff)}')\n",
    "    print(f'mse: {mse}')\n",
    "    return prediction,mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
